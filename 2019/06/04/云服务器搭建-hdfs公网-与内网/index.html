<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title>云服务器搭建 hdfs,公网 与内网 | Hexo</title><meta name="description" content="云服务器搭建 hdfs,公网 与内网 - John Doe"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/theme.css"><link rel="search" type="application/opensearchdescription+xml" href="/atom.xml" title="Hexo"><link rel="stylesheet" href="/css/font-awesome.min.css"><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head><body><div class="wrap"><header><h1 class="branding"><a href="/" title="Hexo"><img class="logo-image" src="/favicon.ico" alt="logo"></a></h1><ul class="nav nav-list"><li class="nav-list-item"><!--a.nav-list-link(class={active: act} href=url_for(value), target=tar)--><a class="nav-list-link" href="/" target="_self">首页</a></li><li class="nav-list-item"><!--a.nav-list-link(class={active: act} href=url_for(value), target=tar)--><a class="nav-list-link" href="/archives" target="_self">归档</a></li><li class="nav-list-item"><!--a.nav-list-link(class={active: act} href=url_for(value), target=tar)--><a class="nav-list-link" href="/tags" target="_self">标签</a></li><li class="nav-list-item"><!--a.nav-list-link(class={active: act} href=url_for(value), target=tar)--><a class="nav-list-link" href="/about" target="_self">关于</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">云服务器搭建 hdfs,公网 与内网</h1><p class="post-info"><i class="fa fa-calendar">&nbsp;</i>2019-06-04&nbsp;| <i class="fa fa-tags">&nbsp;</i><a class="post-tag-link" href="/tags/hdfs/">hdfs&nbsp;·&nbsp;</a><a class="post-tag-link" href="/tags/%E5%85%AC%E7%BD%91/">公网&nbsp;·&nbsp;</a><a class="post-tag-link" href="/tags/%E8%85%BE%E8%AE%AF%E4%BA%91/">腾讯云&nbsp;·&nbsp;</a><a class="post-tag-link" href="/tags/%E9%98%BF%E9%87%8C%E4%BA%91/">阿里云&nbsp;·&nbsp;</a>&nbsp;| <i class="fa fa-folder-o">&nbsp;</i><a class="post-category-link" href="/categories/program/">program&nbsp;/&nbsp;</a><a class="post-category-link" href="/categories/program/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据&nbsp;/&nbsp;</a></p><div class="post-content"><p>from <a target="_blank" rel="noopener" href="https://www.cnblogs.com/ya-qiang/p/10076424.html">https://www.cnblogs.com/ya-qiang/p/10076424.html　</a> 这个问题花费了我将近两天的时间，经过多次试错和尝试，现在想分享给大家来解决此问题避免大家入坑，以前都是在局域网上搭建的hadoop集群，并且是局域网访问的，没遇见此问题。 因为阿里云上搭建的hadoop集群，需要配置映射集群经过内网访问，也就是局域网的ip地址。 如果配置为公网IP地址，就会出现集群启动不了，namenode和secondarynamenode启动不了，如果将主机的映射文件配置为内网IP集群就可以正常启动了。但通过eclipse开发工具访问 会出错，显示了阿里云内网的ip地址来访问datanode，这肯定访问不了啊，这问题真实醉了，就这样想了找了好久一致没有思路。 <strong>最终发现需要在hdfs-site.xml中修改配置项dfs.client.use.datanode.hostname设置为true，就是说客户端访问datanode的时候是通过主机域名访问，就不会出现通过****内网IP来访问了</strong> 最初查看日志发现：</p>
<h2 id="一、查看日志"><a href="#一、查看日志" class="headerlink" title="一、查看日志"></a>一、查看日志</h2><h3 id="1-less-hadoop-hadoop-namenode-master-log"><a href="#1-less-hadoop-hadoop-namenode-master-log" class="headerlink" title="1. less hadoop-hadoop-namenode-master.log"></a>1. less hadoop-hadoop-namenode-master.log</h3><p><img src="https://img2018.cnblogs.com/blog/1256944/201812/1256944-20181206133008238-2049594469.png"></p>
<h2 id="2-less-hadoop-hadoop-secondarynamenode-master-log"><a href="#2-less-hadoop-hadoop-secondarynamenode-master-log" class="headerlink" title="2.less hadoop-hadoop-secondarynamenode-master.log"></a>2.less hadoop-hadoop-secondarynamenode-master.log</h2><p><img src="https://img2018.cnblogs.com/blog/1256944/201812/1256944-20181206133103621-1924499106.png"></p>
<h2 id="二、解决集群访问问题"><a href="#二、解决集群访问问题" class="headerlink" title="二、解决集群访问问题"></a>二、解决集群访问问题</h2><h3 id="1-查看hosts映射文件"><a href="#1-查看hosts映射文件" class="headerlink" title="1.查看hosts映射文件"></a>1.查看hosts映射文件</h3><p><img src="https://img2018.cnblogs.com/blog/1256944/201812/1256944-20181206134128416-793205993.png"> <strong>上面是公网IP需要替换为内网IP</strong> <img src="https://img2018.cnblogs.com/blog/1256944/201812/1256944-20181206135623882-1110697964.png"> 然后正常搭建hadoop集群</p>
<h3 id="2-core-site-xml"><a href="#2-core-site-xml" class="headerlink" title="2.core-site.xml"></a>2.core-site.xml</h3><p><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></p>
<!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 -->
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://master:9000</value>
</property>
<!-- 指定hadoop运行时产生文件的存储目录 -->
<property>
    <name>hadoop.tmp.dir</name>
    <value>/home/hadoop/BigData/hadoop-2.7.3/data</value>
</property>

<p><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></p>
<h3 id="3-hadoop-env-sh-修改export-JAVA-HOME值"><a href="#3-hadoop-env-sh-修改export-JAVA-HOME值" class="headerlink" title="3.hadoop-env.sh 修改export JAVA_HOME值"></a>3.hadoop-env.sh 修改export JAVA_HOME值</h3><p>export JAVA_HOME=/home/hadoop/BigData/jdk1.8</p>
<h3 id="4-hdfs-site-xml-注意：添加一个dfs-client-use-datanode-hostname配置"><a href="#4-hdfs-site-xml-注意：添加一个dfs-client-use-datanode-hostname配置" class="headerlink" title="4.hdfs-site.xml 注意：添加一个dfs.client.use.datanode.hostname配置"></a><strong>4.hdfs-site.xml 注意：添加一个dfs.client.use.datanode.hostname配置</strong></h3><p><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></p>
<!-- 指定namenode的http通信地址 -->
<property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>master:50090</value>
</property>
<!-- 指定HDFS副本的数量 -->
<property>
    <name>dfs.replication</name>
    <value>1</value>
</property>
<property>
    <name>dfs.permissions</name>
    <value>false</value>
</property>
<!-- 如果是通过公网IP访问阿里云上内网搭建的集群 -->
<property>
    <name>dfs.client.use.datanode.hostname</name>
    <value>true</value>
    <description>only cofig in clients</description>
</property>

<p><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></p>
<h3 id="5-mapred-site-xml"><a href="#5-mapred-site-xml" class="headerlink" title="5.mapred-site.xml"></a>5.mapred-site.xml</h3><p><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></p>
<!-- 指定mr运行在yarn上 -->
<property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
</property>
<!-- jobhistory的address -->
<property>
    <name>mapreduce.jobhistory.address</name>
    <value>master:10020</value>
</property>
<!-- jobhistory的webapp.address -->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>master:19888</value>
</property>

<p><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></p>
<h3 id="6-yarn-site-xml"><a href="#6-yarn-site-xml" class="headerlink" title="6. yarn-site.xml"></a>6. yarn-site.xml</h3><p><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></p>
<!-- 指定YARN的老大（ResourceManager）的地址 -->
<property>
    <name>yarn.resourcemanager.hostname</name>
    <value>master</value>
</property>
<!-- reducer获取数据的方式 -->
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce\_shuffle</value>
</property>

<p><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></p>
<h3 id="7-hadoop-namenode-format格式化，然后启动start-all-sh"><a href="#7-hadoop-namenode-format格式化，然后启动start-all-sh" class="headerlink" title="7.hadoop namenode -format格式化，然后启动start-all.sh"></a>7.hadoop namenode -format格式化，然后启动start-all.sh</h3><p><img src="https://img2018.cnblogs.com/blog/1256944/201812/1256944-20181206135712531-1344256009.png"></p>
<h3 id="8-在本地IDE环境中编写单词统计测试集群访问"><a href="#8-在本地IDE环境中编写单词统计测试集群访问" class="headerlink" title="8.在本地IDE环境中编写单词统计测试集群访问"></a>8.在本地IDE环境中编写单词统计测试集群访问</h3><p><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></p>
<p>public class WordCount {<br>    public static class TokenizerMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{<br>        private final static IntWritable one = new IntWritable(1);<br>        private Text word = new Text();<br>        @Override<br>        protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context)<br>                throws IOException, InterruptedException {<br>            StringTokenizer itr = new StringTokenizer(value.toString());<br>            while(itr.hasMoreTokens()) {<br>                word.set(itr.nextToken());<br>                context.write(word, one);<br>            }<br>        }</p>
<pre><code>    public static class WordCountReducer extends Reducer&lt;Text, IntWritable, Text,IntWritable&gt;&#123;
        private IntWritable result = new IntWritable();
        @Override
        protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,
                Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)
                throws IOException, InterruptedException &#123;
                int sum = 0;
                for(IntWritable item:values) &#123;
                    sum += item.get();
                &#125;
                result.set(sum);
                context.write(key, result);
        &#125;
    &#125;
    public static void main(String\[\] args) throws IOException, ClassNotFoundException, InterruptedException &#123;
        Configuration conf = new Configuration();
        String\[\] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
        if(otherArgs.length &lt; 2) &#123;
            System.err.println(&quot;Usage: wordcount &lt;in&gt; \[&lt;in&gt;....\] &lt;out&gt;&quot;);
            System.exit(2);
        &#125;
        Job job = Job.getInstance(conf, &quot;word count&quot;);
        job.setJarByClass(WordCount.class);
        job.setMapperClass(TokenizerMapper.class);
        job.setCombinerClass(WordCountReducer.class);
        job.setReducerClass(WordCountReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        for(int i = 0; i &lt; otherArgs.length -1; i++) &#123;
            FileInputFormat.addInputPath(job, new Path(otherArgs\[i\]));
        &#125;
        FileSystem fs = FileSystem.get(conf);
        Path output = new Path(otherArgs\[otherArgs.length - 1\]);
        if(fs.exists(output)) &#123;
            fs.delete(output, true);
            System.out.println(&quot;output directory existed! deleted!&quot;);
        &#125;
        FileOutputFormat.setOutputPath(job, output);
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    &#125;
&#125;
</code></pre>
<p>}</p>
<p><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></p>
<h3 id="9-运行的时候配置一个数据的存放路径和数据的输出路径位置"><a href="#9-运行的时候配置一个数据的存放路径和数据的输出路径位置" class="headerlink" title="9.运行的时候配置一个数据的存放路径和数据的输出路径位置"></a>9.运行的时候配置一个数据的存放路径和数据的输出路径位置</h3><p><img src="https://img2018.cnblogs.com/blog/1256944/201812/1256944-20181206140115136-910093063.png"></p>
<h3 id="10-正常运行并访问了阿里云的hadoop集群"><a href="#10-正常运行并访问了阿里云的hadoop集群" class="headerlink" title="10 . 正常运行并访问了阿里云的hadoop集群"></a>10 . 正常运行并访问了阿里云的hadoop集群</h3></div></article></div><div class="article"><hr><h2>版权声明</h2>| 文章作者：<a target="_blank" rel="noopener" href="http://wwwuxt.cc">wwwuxt</a><br>| 文章链接：<a href="http://example.com/2019/06/04/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA-hdfs%E5%85%AC%E7%BD%91-%E4%B8%8E%E5%86%85%E7%BD%91/">http://example.com/2019/06/04/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA-hdfs%E5%85%AC%E7%BD%91-%E4%B8%8E%E5%86%85%E7%BD%91/</a><br>| 许可协议：<a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a><hr></div></main><footer><div class="paginator"><a class="prev" href="/2019/06/20/code-for-presto-testcase/"><!--!= __('< PREV_POST')-->< Code for Presto TestCase</a><a class="next" href="/2019/05/24/centos7-%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8-%E8%BF%9B%E5%85%A5emergency-mode/"><!--!= __('NEXT_POST >')-->CentOS7 无法启动 进入emergency mode ></a></div><div class="clearfix"></div><div class="copyright"><p id="host_by"> <a href="/atom.xml"><i class="fa fa-rss"></i></a><span id="busuanzi_container_site_pv">&nbsp;<i class="fa fa-eye">&nbsp;</i><span id="busuanzi_value_site_pv"><i class="fa fa-spinner"></i></span> times, </span><span id="busanzi_container_site_uv">&nbsp;<i class="fa fa-user">&nbsp;</i><span id="busuanzi_value_site_uv"><i class="fa fa-spinner"></i></span> times.</span><br> &copy; 2017 - 2021 <a target="_blank" rel="noopener" href="http://wwwuxt.cc">wwwuxt</a>. 
 Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/wwwuxt/hexo-theme-artemisX" target="_blank">ArtemisX</a>.<br></p><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></footer></div><script>var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-108353521-1']);
_gaq.push(['_trackPageview']);

(function () {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
console.log('Google Analytics')
</script><script>(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https'){
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else{
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
    console.log('wwwuxt.cc')
})();
</script><script>(function(){
    var req = GetXmlHttpObject()  
    if (req == null) {  
        console.log("not support AJAX!");  
        return;  
    }
    req.onreadystatechange = function() {  
        if (req.readyState === 4 && req.status === 200) {  
            var deploy_server = req.getResponseHeader("Server");
            console.log(deploy_server)
            if(deploy_server === 'Coding Pages'){
                document.getElementById('host_by').innerHTML+='Hosted by <a target="_blank" rel="noopener" href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a>.'
            }
            else if(deploy_server === 'GitHub.com'){
                document.getElementById('host_by').innerHTML+='Hosted by <a target="_blank" rel="noopener" href="https://pages.github.com" style="font-weight: bold">GitHub Pages</a>.'
            }
            else{
                document.getElementById('host_by').innerHTML+='Hosted by <a href="#" style="font-weight: bold">'+ deploy_server + '</a>.'
            }
        }  
    };  
    req.open('GET', document.location, true);
    req.send(null);
})();
function GetXmlHttpObject() {  
    var xmlHttp = null;  
    try {  
        // Firefox, Opera 8.0+, Safari  
        xmlHttp = new XMLHttpRequest();  
    } catch (e) {  
        // Internet Explorer  
        try {  
            xmlHttp = new ActiveXObject("Msxml2.XMLHTTP");  
        } catch (e) {  
            xmlHttp = new ActiveXObject("Microsoft.XMLHTTP");  
        }  
    }  
    return xmlHttp;  
}  </script></body></html>